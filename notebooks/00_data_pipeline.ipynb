{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fe8511",
   "metadata": {},
   "source": [
    "# Data Pipeline: RAW → INTERIM → PROCESSED\n",
    "\n",
    "This notebook demonstrates the complete data preprocessing pipeline:\n",
    "\n",
    "1. **RAW**: Load original untouched data from `data/raw/`\n",
    "2. **INTERIM**: Engineer features and save to `data/interim/`\n",
    "3. **PROCESSED**: Apply scaling/encoding and save to `data/processed/`\n",
    "\n",
    "All steps are validated and logged for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.utils.seed import set_seed\n",
    "from src.utils.config import Config\n",
    "from src.preprocessing import DataPipeline\n",
    "\n",
    "set_seed(42)\n",
    "print(\"✓ Imports and seed configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b9fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('../configs/config.yaml')\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  RAW: {config.data['raw_data_path']}\")\n",
    "print(f\"  INTERIM: {config.data['interim_data_path']}\")\n",
    "print(f\"  PROCESSED: {config.data['processed_data_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = DataPipeline(config)\n",
    "\n",
    "print(\"Pipeline initialized!\")\n",
    "print(\"Ready to execute: RAW → INTERIM → PROCESSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete pipeline\n",
    "raw_df, interim_df, processed_df = pipeline.run()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RAW data shape: {raw_df.shape}\")\n",
    "print(f\"INTERIM data shape: {interim_df.shape}\")\n",
    "print(f\"PROCESSED data shape: {processed_df.shape}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67a3d4",
   "metadata": {},
   "source": [
    "## Validation Results\n",
    "\n",
    "The pipeline includes automatic validation:\n",
    "- ✓ Required columns exist\n",
    "- ✓ Target column present\n",
    "- ✓ No duplicate column names\n",
    "- ✓ Missing value analysis\n",
    "- ✓ Data type verification\n",
    "- ✓ Safety checks (prevents writing to raw/)\n",
    "\n",
    "All validation results are logged above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60adb3",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that data is processed, you can:\n",
    "1. Run `01_data_exploration.ipynb` to explore features\n",
    "2. Run `02_baseline_training.ipynb` to train baseline models\n",
    "3. Continue with advanced models in subsequent notebooks\n",
    "\n",
    "All notebooks will automatically load from `data/processed/feature_matrix_processed.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad70051",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ Data pipeline complete!\")\n",
    "print(\"Proceed to exploratory data analysis and model training.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
