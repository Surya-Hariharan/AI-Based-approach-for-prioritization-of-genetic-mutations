{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.utils.seed import set_seed\n",
    "from src.utils.config import Config\n",
    "from src.preprocessing import get_data_loaders\n",
    "from src.models import MLP\n",
    "from src.evaluation import calculate_metrics\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('../configs/config.yaml')\n",
    "train_loader, val_loader, test_loader, input_dim = get_data_loaders(config)\n",
    "\n",
    "print(f\"Input Features: {input_dim}\")\n",
    "print(f\"Train Samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Val Samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Test Samples: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(\n",
    "    input_dim=input_dim,\n",
    "    hidden_layers=config.model['mlp']['hidden_layers'],\n",
    "    dropout=config.model['mlp']['dropout']\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.training['learning_rate'])\n",
    "\n",
    "print(f\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735822af",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = config.training['epochs']\n",
    "patience = config.training.get('early_stopping_patience', 5)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_aucs = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for X_batch, y_batch in progress_bar:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device).float().unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_metrics = calculate_metrics(np.array(all_labels), np.array(all_probs))\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_aucs.append(val_metrics['auc'])\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val AUC: {val_metrics['auc']:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), '../reports/results/checkpoints/mlp_best.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n✓ Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524260f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_trained = len(train_losses)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(range(1, epochs_trained+1), train_losses, label='Train Loss', marker='o')\n",
    "axes[0].plot(range(1, epochs_trained+1), val_losses, label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('MLP Model: Training Progress')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(1, epochs_trained+1), val_aucs, label='Val AUC', marker='o', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('AUC')\n",
    "axes[1].set_title('Validation AUC Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd01b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../reports/results/checkpoints/mlp_best.pth'))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        \n",
    "        all_probs.extend(probs.flatten())\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "all_probs = np.array(all_probs)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "test_metrics = calculate_metrics(all_labels, all_probs)\n",
    "\n",
    "print(\"MLP Model Test Results (Best Checkpoint):\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix, auc\n",
    "import seaborn as sns\n",
    "\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(all_labels, all_probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(fpr, tpr, lw=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve - MLP Model')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(recall, precision, lw=2, label=f'PR (AUC = {pr_auc:.3f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "cm = confusion_matrix(all_labels, (all_probs > 0.5).astype(int))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[2])\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('Actual')\n",
    "axes[2].set_title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4846bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ MLP training complete!\")\n",
    "print(f\"Best model saved at: reports/results/checkpoints/mlp_best.pth\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
