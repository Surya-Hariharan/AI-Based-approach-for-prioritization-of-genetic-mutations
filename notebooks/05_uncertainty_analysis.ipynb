{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17379223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.utils.seed import set_seed\n",
    "from src.utils.config import Config\n",
    "from src.preprocessing import get_data_loaders\n",
    "from src.models import MLP\n",
    "from src.uncertainty import MCDropoutEstimator, BayesianRanker\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ee53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('../configs/config.yaml')\n",
    "train_loader, val_loader, test_loader, input_dim = get_data_loaders(config)\n",
    "\n",
    "model = MLP(\n",
    "    input_dim=input_dim,\n",
    "    hidden_layers=config.model['mlp']['hidden_layers'],\n",
    "    dropout=config.model['mlp']['dropout']\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('../reports/results/checkpoints/mlp_best.pth'))\n",
    "print(\"✓ Loaded trained MLP model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_estimator = MCDropoutEstimator(\n",
    "    model=model,\n",
    "    n_samples=config.uncertainty['mc_dropout_samples'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Running MC Dropout inference...\")\n",
    "\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for X, y in test_loader:\n",
    "    X_test_list.append(X)\n",
    "    y_test_list.append(y)\n",
    "\n",
    "X_test = torch.cat(X_test_list)\n",
    "y_test = torch.cat(y_test_list)\n",
    "\n",
    "mean_preds, var_preds = mc_estimator.predict(X_test, return_variance=True)\n",
    "\n",
    "print(\"✓ Uncertainty estimation complete!\")\n",
    "print(f\"Mean predictions shape: {mean_preds.shape}\")\n",
    "print(f\"Variance shape: {var_preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(mean_preds, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Predicted Probability')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Prediction Distribution (MC Dropout)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(mean_preds, var_preds, alpha=0.5)\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Predictive Variance')\n",
    "axes[1].set_title('Uncertainty vs Prediction')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = BayesianRanker(confidence_strength=10)\n",
    "\n",
    "posterior_mean, lower_bound, upper_bound = ranker.compute_posterior(mean_preds, var_preds)\n",
    "ranked_indices = ranker.rank(mean_preds, var_preds)\n",
    "\n",
    "print(\"Top 10 ranked variants (with uncertainty):\")\n",
    "for i, idx in enumerate(ranked_indices[:10]):\n",
    "    print(f\"{i+1}. Index {idx}: Mean={posterior_mean[idx]:.4f}, CI=[{lower_bound[idx]:.4f}, {upper_bound[idx]:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51417911",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_scores = ranker.confidence_score(mean_preds, var_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(confidence_scores, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Confidence Score Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean confidence: {confidence_scores.mean():.4f}\")\n",
    "print(f\"Std confidence: {confidence_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc33f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ Uncertainty analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
